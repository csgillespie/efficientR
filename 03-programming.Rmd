---
knit: "bookdown::preview_chapter"
---

# Efficient programming {#programming}
 
Many people who use R would not describe themselves as "programmers". Instead they tend to
have advanced domain level knowledge, understand standard R data
structures, such as vectors and data frames, but have little formal training in computing.
Sound familiar? In that case this chapter is for you.

In this
chapter we will discuss "big picture" programming techniques. We cover general concepts and R programming
techniques about code optimisation, before describing idiomatic programming
structures. We conclude the chapter by examining relatively easy ways of speeding up
code using the **compiler** package and parallel processing, using multiple CPUs.

## Top 5 tips for efficient programming

  * Be careful never to grow vectors.
  * Use `invisible` to return potentially useful information.
  * Use factors when appropriate.
  * Clever use of S3 objects can make code easier to understand.
  * Byte compile packages for an easy performance boost.

<!-- Weak title -->
## General advice

Low level languages like C and Fortran demand more from the programmer. They force you to declare the type of
every variable used and give you the burdensome responsibility of memory management. The
advantage of such languages, compared with R, is that they are faster to run.
The disadvantage is that they take longer to learn and type.

```{block, type="rmdnote"}
The wikipedia page on compiler opimisations gives a nice overview of standard
optimisation techniques (https://en.wikipedia.org/wiki/Optimizing_compiler).
```

R users don't tend to worry about data types.
This is advantageous in terms of creating concise code, but can result in R programs that are slow. While optimisations such as going
parallel can double speed, poor code can easily run 100's of times slower, so it's important to understand the causes of slow code. These are covered in @Burns2011, which should be considered essential
reading for any aspiring R programmers.

Ultimately calling an R function always ends up calling some underlying C/Fortran
code. For example the base R function `runif` only contains a single line that
consists of a call to `C_runif`.

```{r eval=TRUE, results="hide"}
function (n, min = 0, max = 1) 
  .Call(C_runif, n, min, max)
```

A **golden rule** in R programming is to access the underlying C/Fortran routines as
quickly as possible; the fewer functions calls required to achieve this, the better.
For example, suppose `x` is a standard vector of length `n`. Then
```{r echo=3}
n = 2
x = runif(n)
x = x + 1
```
involves a single function call to the `+` function. Whereas the `for` loop
```{r bad_loop}
for(i in 1:n) 
  x[i] = x[i] + 1 
```
has

  * `n` function calls to `+`;
  * `n` function calls to the `[` function;
  * `n` function calls to the `[<-` function (used in the assignment operation);
  *  A function call to `for` and to the `:` operator. 

It isn't that the `for` loop is slow, rather it is because we have many more function
calls. Each individual function call is quick, but the total combination is slow.

```{block, type="rmdnote"}
Everything in R is a function call. When we execute `1 + 1`, we are actually
executing `+(1, 1)`.
```

#### Exercise {-}

Use the **microbenchmark** package to compare the vectorised construct `x = x + 1`, to the
`for` loop version. Try varying the size of the input vector.

### Memory allocation

Another general technique is to be careful with memory allocation. If possible
pre-allocate your vector then fill in the values.

```{block, type="rmdtip"}
You should also consider pre-allocating memory for data frames and lists. Never grow
an object. A good rule of thumb is to compare your objects before and after a `for`
loop; have they increased in length?
```

Let's consider three methods of creating a sequence of numbers. __Method 1__ creates
an empty vector and grows the object

```{r echo=TRUE, tidy=FALSE}
method1 = function(n) {
  vec = NULL # Or vec = c()
  for(i in 1:n)
    vec = c(vec, i)
  vec
}
```
__Method 2__ creates an object of the final length and then changes the values in the
object by subscripting:
```{r echo=TRUE, tidy=FALSE}
method2 = function(n) {
  vec = numeric(n)
  for(i in 1:n)
    vec[i] = i
  vec
}
```
__Method 3__ directly creates the final object
```{r eval=TRUE, echo=TRUE}
method3 = function(n) 1:n
```
To compare the three methods we use the `microbenchmark` function from the previous chapter
```{r tidy=FALSE,eval=FALSE}
microbenchmark(times = 100, unit="s",
                               method1(n), method2(n), method3(n))
```

The table below shows the timing in seconds on my machine for these three methods for
a selection of values of `n`. The relationships for varying `n` are all roughly linear
on a log-log scale, but the timings between methods are drastically different. Notice
that the timings are no longer trivial. When $n=10^7$, method $1$ takes around an hour
whilst method $2$ takes $2$ seconds and method $3$ is almost instantaneous. Remember
the golden rule; access the underlying C/Fortran code as quickly as possible.

$n$ | Method 1 | Method 2 | Method 3 
----|----------|----------|---------
$10^5$ | $\phantom{000}0.21$    | $0.02$ | $0.00$
$10^6$ | $\phantom{00}25.50$    | $0.22$ | $0.00$
$10^7$ | $3827.00$              | $2.21$ | $0.00$

Table: Time in seconds to create sequences. When $n=10^7$, method $1$ takes around an
hour while the other methods take less than $3$ seconds.

### Vectorised code

The vector is one of the key data types in R, with many functions offering a
vectorised version. For example, the code
```{r, echo=2}
n = 10
x = runif(n) + 1
```
performs two vectorised operations. First `runif` returns `n` random numbers. Second
we add `1` to each element of the vector. In general it is a good idea to exploit
vectorised functions. Consider this piece of R code that calculates the sum of
$\log(x)$
```{r eval=FALSE, echo=TRUE, tidy=FALSE}
log_sum = 0
for(i in 1:length(x))
  log_sum = logsum + log(x[i])
```

```{block, type="rmdwarning"}
Using `1:length(x)` can lead to hard-to-find bugs when `x` has length zero. Instead
use `seq_along(x)` or `seq_len(length(x))`.
```
This code could easily be vectorised via
```{r eval=TRUE}
log_sum = sum(log(x))
```
Writing code this way has a number of benefits.

  * It's faster. When $n = 10^7$ the ``R way'' is about forty times faster.
  * It's neater.
  * It doesn't contain a bug when `x` is of length $0$.

#### Exercises {-}

Time the two methods for calculating the log sum. Try different values of $n$.

#### Example: Monte-Carlo integration {-}

It's also important to make full use of R functions that use vectors. For example,
suppose we wish to estimate the integral
\[
\int_0^1 x^2 dx
\]
using a Monte-Carlo method. Essentially, we throw darts at the curve and count
the number of darts that fall below the curve (as in \@ref(fig:6-2)).

_Monte Carlo Integration_

1. Initialise: `hits = 0`
1. __for i in 1:N__
1. $~~~$ Generate two random numbers, $U_1, U_2$,  between 0 and 1
1. $~~~$ If $U_2 < U_1^2$, then `hits = hits + 1`
1. __end for__
1. Area estimate = `hits/N`

Implementing this Monte-Carlo algorithm in R would typically lead to something like:
```{r tidy=FALSE}
monte_carlo = function(N){
  hits = 0
  for(i in 1:N)  {
    u1 = runif(1); u2 = runif(1)
    if(u1^2 > u2)
      hits = hits + 1
  }
  return(hits/N)
}
```
In R this takes a few seconds
```{r cache=TRUE}
N = 500000
system.time(monte_carlo(N))
```
In contrast a more R-centric approach would be
```{r echo=TRUE}
monte_carlo_vec = function(N) sum(runif(N)^2 > runif(N))/N
```

The `monte_carlo_vec` function contains (at least) four aspects of vectorisation

  * The `runif` function call is now fully vectorised;
  * We raise entire vectors to a power via `^`;
  * Comparisons using `>` are vectorised;
  * Using `sum` is quicker than an equivalent for loop.

The function `monte_carlo_vec` is around $30$ times faster than `monte_carlo`.
```{r 6-2, fig.cap="Example of Monte-Carlo integration. To estimate the area under the curve throw random points at the graph and count the number of points that lie under the curve.", echo=FALSE,fig.width=6, fig.height=4, fig.align="center", out.width="0.7\\textwidth"}
local(source("code/03-programming_f1.R", local=TRUE))
```

### Exercise {-}

Verify that `monte_carlo_vec` is faster than `monte_carlo`. How does this relate to 
the number of darts, i.e. the size of `N`, that is used

### Type consistency

When programming it is helpful if the return value from a function always takes the
same form. Unfortunately, not all of base R functions follow this idiom. For example
the functions `sapply` and `[.data.frame` aren't type consistent
```{r, results="hide"}
two_cols = data.frame(x = 1:5, y = letters[1:5])
zero_cols = data.frame()
sapply(two_cols, class)  # a character vector
sapply(zero_cols, class) # a list
two_cols[, 1:2]          # a data.frame
two_cols[, 1]            # an integer vector
```
This can cause unexpected problems. The functions `lapply` and `vapply` are type
consistent. Likewise `dplyr::select` and `dplyr:filter`. The **purrr** package has
some type consistent alternatives to base R functions. For example, `map_dbl` etc. to
replace `Map` and `flatten_df` to replace `unlist`.

#### Exercises {-}

1. Rewrite the `sapply` function calls above using `vapply` to ensure type consistency.

1. How would you make subsetting data frames with `[` type consistent? Hint: look at
the `drop` argument.

## Communicating with the user

When we create a function we often want the function to give feedback on the current
state. For example, are there missing arguments or has a numerical calculation failed.
There are three main techniques of communicating with the user.

### Fatal errors: `stop` {-}

Fatal errors are raised by calling the `stop`, i.e. execution is terminated. When
`stop` is called, there is no way for a function to continue. For instance, when we
generate random numbers using `rnorm` the first argument is the sample size,`n`. If the number of
observations to return less than $1$, an error is raised.

Errors can be caught using `try` and `tryCatch`. For example, 
```{r}
# Suppress the error message
good = try(1 + 1, silent = TRUE)
bad = try(1 + "1", silent = TRUE)
```
When we inspect the objects, the variable `good` just contains the number `2`
```{r}
good
```
However, the `bad` object is a character string with class `try-error` and a `condition`
attribute that contains the error message
```{r}
bad
```
We can use this information in a standard conditional statement
```{r eval=FALSE}
if(class(bad) == "try-error")
  # Do something 
```
Further details on error handling, as well as some excellent advice on general
debugging techniques, are given in [@Wickham2014].

### Warnings: `warning` {-}

Warnings are generated using the `warning` function. When a warning is raised, it
indicates potential problems. For example, `mean(NULL)` returns `NA` and also raises a
warning. Warnings can be hidden using `suppressWarnings`:
```{r}
suppressWarnings(mean(NULL))
```
In general, it is good practice to solve the underlying cause of the warning message,
instead of evading the issue via `suppressWarning`.
  
### Informative output: `message` and `cat` {-}  
  
To give informative output, use the `message` function. In my package for fitting
powerlaws, I use messages to give the user an estimate of expected run time. Similar
to warnings, messages can be suppressed with `suppressMessages`.
  
Another function used for printing messages is `cat`. In general `cat` should only be
used in `print`/`show` methods, e.g. look at the function definition of the
S3 print method for `difftime` objects, `getS3method("print", "difftime")`.

### Example: Retrieving a web resource

In some cases it isn't clear what should be returned, a message, a `NA` value or an
error. For example, suppose we wish to download data from a web site. 

The function `GET` from the **httr** package can be used to download a webpage. When
we attempt to download data, the function also obtains an HTML status code. If the
page exists, the function will return text (with the correct status code), e.g.
```{r eval=FALSE}
GET("http://google.com/") # Status: 200
```
If the url is incorrect, e.g. a broken link or is a redirect, then we can interrogate the
output and use the Status to determine what to do next.
```{r eval=FALSE}
GET("http://google.com/made_up") # Status: 404
```
By examining the error code, the user has complete flexibility. However if the
web-page doesn't exist
```{r eval=FALSE}
GET("http://google1.com/")
```
or if we don't have an internet connection, then `GET` raises an error via `stop`.
Instead of raising an error the author could have returned `NA` (the web-page isn't
available). However for the typically use case, it's not clear how returning `NA`
would be helpful.

The `install_github` function in the **devtools** package either installs the
package or raises an error. When an error is raised, the function uses a combination
of `message` and `stop` to indicate the source of an error, e.g.
```{r, eval=FALSE}
devtools::install_github("bad/package")
#> Downloading GitHub repo bad/package@master
#> from URL https://api.github.com/repos/bad/package/zipball/master
#>  Error in curl::curl_fetch_memory(url, handle = handle) : 
#>   Couldn't resolve host name 
```
With the key idea being that if a package isn't able to be installed, future
calculations will break; so it's optimal to raise an error as soon as possible.

<!--
When deciding what to return, consider what the user can 
The key idea is
what can the user do if the function cannot complete it's purpose.
-->

### Exercises {-}

The `stop` function has an argument `call.` that indicates if the function call
should be part of the error message. Create a function and experiment with this option.

### Invisible returns 

The `invisible` function allows you to return a temporarily invisible copy of an
object. This is particularly useful for functions that return values which can be
assigned, but are not printed when they are not assigned. For example suppose we have
a function that plots the data and fits a straight line
```{r}
regression_plot = function(x, y, ...) {
  # Plot and pass additional arguments to default plot method
  plot(x, y, ...) 
  
  # Fit regression model 
  model = lm(y ~ x)
  # Add line of best fit to the plot
  abline(model)
  invisible(model)
}
```
When the function is called, a scatter graph is plotted with the line of best fit, but
the output is invisible. However when we assign the function to an object, i.e. 
`out = regression_plot(x, y)` the variable `out` contains the output of the `lm` call.

Another example is `hist`. Typically we don't want anything displayed in the console
when we call the function
```{r fig.keep="none", echo=2}
x = rnorm(x)
hist(x)
```
However if we assign the output to an object, `out = hist(x)`, the object `out` is
actually a list containing, _inter alia_, information on the mid-points, breaks and
counts. 

## Factors

Factors are much maligned objects. While at times they are awkward, they do have their
uses. A factor is used to store categorical variables. This data type is unique to R
(or at least not common among programming languages). Often categorical variables get
stored as $1$, $2$, $3$, $4$, and $5$, with associated documentation elsewhere that
explains what each number means. This is clearly a pain. Alternatively we store the
data as a character vector. While this is fine, the semantics are wrong because it
doesn't convey that this is a categorical variable. It's not sensible to say that you
should **always** or **never** use factors, since factors have both positive and
negative features. Instead we need to examine each case individually. As a guide of
when it's appropriate to use factors, consider the following examples.

### Example: Months of the year

Suppose our data set relates to months of the year

```{r}
m = c("January", "December", "March")
```

If we sort `m` in the usual way, `sort(m)`, we perform standard alpha-numeric
ordering; placing `December` first. This is technically correct, but not that helpful.
We can use factors to remedy this problem by specifying the admissible levels

```{r}
# month.name contains the 12 months
fac_m = factor(m, levels=month.name)
sort(fac_m)
```

### Example: Graphics

Factors can be used for ordering in graphics. For instance, suppose we have a data set
where the variable `type`, takes one of three values, `small`, `medium` and `large`.
Clearly there is an ordering. Using a standard `boxplot` call, 
```{r fig.keep="none", echo=6}
set.seed(1)
level = c("Small", "Medium", "Large")
type = rep(level, each=30)
y = rnorm(90)
type_factor = factor(type, levels=level)
boxplot(y ~ type)
```
would create a boxplot where the $x$-axis was alphabetically ordered. By converting
`type` into factor, we can easily specify the correct ordering.
```{r, boxplot_factor, eval=TRUE, efig.keep="none"}
boxplot(y ~ factor(type, levels=c("Small", "Medium", "Large")))
```

### Example: Analysis of variance

Analysis of variance (ANOVA) is a type of statistical model that is used to determine
differences among group means, while taken into account other factors. The function
`aov` is used to fit standard analysis of variance models. Potential catastrophic bugs
arise when a variable is numeric, but in reality is a categorical variable.

Consider the `npk` dataset on the growth of peas that comes with R. The column
`block`, indicates the block (typically a nuisance parameter) effect. This column
takes values $1$ to $5$, but has been carefully coded as a factor. Using the `aov`
function to estimate the `block` effect, we get
```{r, echo=2}
data(npk, package="datasets")
aov(yield ~ block, npk)
```
If we repeat the analysis, but change `block` to a numeric data type we get different
(and incorrect) results
```{r}
aov(yield ~ as.numeric(block), npk)
```
When we pass a numeric variable, the `aov` function is interpreting this variable 
as continuous, and fits a regression line.

### Example: data input

Most users interact with factors via the `read.csv` function where character columns
are automatically converted to factors. This feature can be irritating if our data is
messy and we want to clean and recode variables. Typically when reading in data via
`read.csv`, we use the `stringsAsFactors=FALSE` argument.
```{block, type="rmdwarning"}
Although this argument can add in the global `options()` list and placed in the
`.Rprofile`, this leads to non-portable code, so should be avoided.
```

### Example: Factors are not character vectors

Although factors look similar to character vectors, they are actually integers. This
leads to initially surprising behaviour
```{r}
x = 4:6
c(x)
c(factor(x))
```
In this case the `c` function is using the underlying integer representation of the
factor.


Overall factors are useful, but can lead to unwanted side-effects if we are not
careful. Used at the right time and place, factors can lead to simpler code.

#### Exercise {-}

Factors are slightly more space efficient than characters. Create a character vector
and corresponding factor and use `pryr::object_size` to calculate the space needed for
each object.

```{r echo=FALSE, eval=FALSE}
ch = sample(month.name, 1e6, replace = TRUE)
fac = factor(ch, levels = month.name)
pryr::object_size(ch)
pryr::object_size(fac)
```

## S3 objects {#S3}

R has three built-in object oriented (OO) systems. These systems differ in how classes
and methods are defined. The easiest and oldest system is the S3 system. S3 refers to
the third version of S. The syntax of R is largely based on this version of S. In R
there has never been S1 and S2 classes. The other two OO frameworks are S4 classes
(used mainly in [bioconductor](http://bioconductor.org/) packages) and reference
classes.

```{block, type="rmdnote"}
There are also packages that also provide additional OO frameworks, such as **proto**,
**R6** and **R.oo**. If you are new to OO in R, then S3 is the place to start.
```

In this section we will just discuss the S3 system since that is the most popular. The
S3 system implements a generic-function object oriented (OO) system. This type of OO
is different to the message-passing style of Java and C++. In a message-passing
framework, messages/methods are sent to objects and the object determines which
function to call, e.g. `normal.rand(1)`. The S3 class system is different. In S3, the
_generic_ function decides which method to call - it would have the form `rand(normal,
1)`. By using an OO framework, we avoid an explosion of exposed functions, such as,
`rand_normal`, `rand_uniform`, `rand_poisson` and instead have a single function call
`rand` that passes the object to the correct function.

The S3 system is based on the class of an object. In S3, a class is just an attribute
which can be determined with the `class` function.

```{r echo=2}
data("USArrests", package="datasets")
class(USArrests)
```
The S3 system can be used to great effect. When we pass an object to a _generic_
function, the function first examines the class of the object, and then dispatches the
object to another method. For example, the `summary` function is a S3 generic function
```{r}
functionBody("summary")
```
Note that the only operational line is `UseMethod("summary")`. This handles the method
dispatch based on the object's class. So when `summary(USArrests)` is executed, the
generic `summary` function passes `USArrests` to the function `summary.data.frame`. If
the function `summary.data.frame` does not exist, then `summary.default` is called (if
it exists). If neither function exist, an error is raised.

This simple message passage mechanism enables us to quickly create our own functions.
Consider the distance object:
```{r}
dist_usa = dist(USArrests)
```
The `dist_usa` object has class `dist`. To visualise the distances, we create an
`image` method. First we'll check if the existing `image` function is generic, via
```{r}
# In R3.3, a new function isS3stdGeneric is going to be introduced.
functionBody("image")
```
Since `image` is already a generic method, we just have to create a specific `dist`
method
```{r image_dist_s3}
image.dist = function(x, ...) {
  x_mat = as.matrix(x)
  image(x_mat, main=attr(x, "method"), ...)  
}
```
The `...` argument allows us to pass arguments to the main `image` method, such as
`axes` (see figure \@ref(fig:6-1).

```{r 6-1, fig.cap="S3 image method for data of class `dist`.", echo=FALSE, fig.asp=0.7, fig.width=5,fig.align="center"}
par(mar=c(1, 1, 2, 1), mgp=c(0, 0, 0))
image(dist(USArrests), axes=FALSE)
```

Many S3 methods work in the same way as the simple `image.dist` function created
above: the object is manipulated into a standard format, then passed to the standard
method. Creating S3 methods for standard functions such as `summary`, `mean`, and
`plot` provides a nice uniform interface to a wide variety of data types.

#### Exercises {-}

A data frame is just an R list, with class `data.frame`.

1. Use a combination of `unclass` and `str` on a data frame to confirm that it is
indeed a list.

2. Use the function `length` on a data frame. What is return? Why?



## Caching variables

A straightforward method for speeding up code is to calculate objects once and reuse
the value when necessary. This could be as simple with replacing `log(x)` in multiple
function calls with the object `log_x` that is defined once and reused. This small
saving in time quickly multiplies when the cached variable is used inside a `for`
loop.

A more advanced form of caching is to use the **memoise** package. If a function is
called multiple times with the same input, it may be possible to speed things up by
keeping a cache of known answers that it can retrieve. The **memoise** package allows
us easily store the value of function call and returns the cached result when the
function is called again with the same arguments. This package trades off memory
versus speed, since the memoised function stores all previous inputs and outputs. To
cache a function, we simply pass the function to the **memoise** function.

The classic memoise example is the factorial function. Another example is to limit use
to a web resource. For example, suppose we are developing a shiny (an interactive
graphic) application where the user can fit regression line to data. The user can
remove points and refit the line. An example function would be

```{r}
# Argument indicates row to remove
plot_mpg = function(row_to_remove) {
  data(mpg, package="ggplot2")
  mpg = mpg[-row_to_remove,]
  plot(mpg$cty, mpg$hwy)
  lines(lowess(mpg$cty, mpg$hwy), col=2)
}
```
We can use **memoise** speed up by caching results. A quick benchmark
```{r benchmark_memoise, fig.keep="none", cache=TRUE, results="hide"}
library("memoise")
m_plot_mpg = memoise(plot_mpg)
microbenchmark(times=10, unit="ms", m_plot_mpg(10), plot_mpg(10))
#> Unit: milliseconds
#>            expr   min    lq  mean median    uq   max neval cld
#>  m_plot_mpg(10)  0.04 4e-02  0.07  8e-02 8e-02   0.1    10  a 
#>    plot_mpg(10) 40.20 1e+02 95.52  1e+02 1e+02 107.1    10   b
```
suggests that we can obtain a $100$-fold speed-up.

#### Exercise {-}

Construct a box plot of timings for the standard plotting function and the memoised
version. 

### Function closures

```{block, type="rmdwarning"}
The following section is meant to provide an introduction to function closures with
example use cases. See [@Wickham2014] for a detailed introduction.
```

More advanced caching is available using _function closures_. A closure in R is an
object that contains functions bound to the environment the closure was created in.
Technically all functions in R have this property, but we use the term function
closure to denote functions where the environment is not in `.GlobalEnv`. One of the
environments associated with a function is known as the enclosing environment, that
is, where was the function created. We can determine the enclosing environment using
`environment`:

```{r}
environment(plot_mpg)
```

The `plot_mpg` function's enclosing environment is the `.GlobalEnv`. This is important
for variable scope, i.e. where should be look for a particular object. Consider the
function `f`

```{r}
f = function() {
  x = 5
  function() x
}
```

When we call the function `f`, the object returned is a function. While the enclosing
environment of `f` is `.GlobalEnv`, the enclosing environment of the _returned_
function is something different

```{r}
g = f()
environment(g)
```
When we call this new function `g`, 
```{r}
x = 10
g()
```
The value returned is obtained from `environment(g)` not from the `.GlobalEnv`. This
persistent environment allows us to cache variables between function calls.
```{block type="rmdnote"}
The operator `<<-` makes R search through the parent environments for an existing
defintion. If such a variable is found (and its binding is not locked) then its value
is redefined, otherwise assignment takes place in the global environment.
```

The `simple_counter` function exploits this feature to enable variable caching

```{r}
simple_counter = function() {
  no = 0
  function() {
    no <<- no + 1
    no
  }
}
```
When we call the `simple_counter` function, we retain object values between function
calls

```{r}
sc = simple_counter()
sc()
sc()
```
The key points of the `simple_counter` function are 

  * The `simple_counter` function returns a function;
  * The enclosing environment of `sc` is not `.GlobalEnv`, instead it's the binding 
    environment of `sc`;
  * The function `sc` has an environment that can be used to store/cache values;
  * The operator `<<-` is used to alter the object `no` in the `sc` environment.

#### Example {-}

We can exploit function closures to simplify our code. Suppose we wished to simulate a
games of Snakes and Ladders. We have function that handles the logic of landing on a
snake

```{r}
check_snake = function(square) {
   switch(as.character(square), 
       '16'=6,  '49'=12, '47'=26, '56'=48, '62'=19, 
       '64'=60, '87'=24, '93'=73, '96'=76, '98'=78, 
       square)
}
```
If we then wanted to determine how often we landed on a Snake, we could use a function
closure to easily keep track of the counter.

```{r}
check_snake = function() {
  no_of_snakes = 0
  function(square) {
    new_square = switch(as.character(square), 
       '16'=6,  '49'=12, '47'=26, '56'=48, '62'=19, 
       '64'=60, '87'=24, '93'=73, '96'=76, '98'=78, 
       square)
    no_of_snakes <<- no_of_snakes + (new_square != square)
    new_square
  }
}
```

Keeping the variable `no_of_snakes` attached to the `check_snake` function, enables
us to have cleaner code.

#### Exercise {-}

The following function implements a stop-watch function
```{r}
stop_watch = function() {
  start_time = stop_time = NULL
  start = function() start_time <<- Sys.time()
  stop = function() {
    stop_time <<- Sys.time()
    difftime(stop_time, start_time)
  }
  list(start=start, stop=stop)
}
watch = stop_watch()
```
It contains two functions. One function for starting the timer
```{r}
watch$start()
```
the other for stopping the timer
```{r results="hide"}
watch$stop()
```
Many stop-watches have the ability to measure not only your overall time but also you
individual laps. Add a `lap` function to the `stop_watch` function that will record
individual times, while still keeping track of the overall time.

## The byte compiler

The **compiler** package, written by R Core member Luke Tierney has been part of R
since version 2.13.0. The **compiler** package allows R functions to be compiled,
resulting in a byte code version that may run faster^[The authors have yet to find a
situation where byte compiled code runs significantly slower.]. The compilation
process eliminates a number of costly operations the interpreter has to perform, such
as variable lookup.

Since R 2.14.0, all of the standard functions and packages in base R are pre-compiled
into byte-code. This is illustrated by the base function `mean`:

```{r}
getFunction("mean")
```
The third line contains the `bytecode` of the function. This means that the
**compiler** package has translated the R function into another language that can be
interpreted by a very fast interpreter. Amazingly the **compiler** package is almost
entirely pure R, with just a few C support routines.

### Example: the mean function

The **compiler** package comes with R, so we just need to load the package in the
usual way
```{r}
library("compiler")
```
Next we create an inefficient function for calculating the mean. This function takes
in a vector, calculates the length and then updates the `m` variable.
```{r}
mean_r = function(x) {
  m = 0
  n = length(x)
  for(i in seq_len(n))
    m = m + x[i]/n
  m
}
```
This is clearly a bad function and we should just `mean` function, but it's a useful
comparison. Compiling the function is straightforward
```{r}
cmp_mean_r = cmpfun(mean_r)
```
Then we use the `microbenchmark` function to compare the three variants


<!-- Make n bigger and just copy and paster output -->
```{r results="hide", eval=FALSE}
# Generate some data
x = rnorm(1000)
microbenchmark(times=10, unit="ms", # milliseconds
          mean_r(x), cmp_mean_r(x), mean(x))
#> Unit: milliseconds
#>           expr   min    lq  mean median    uq  max neval cld
#>      mean_r(x) 0.358 0.361 0.370  0.363 0.367 0.43    10   c
#>  cmp_mean_r(x) 0.050 0.051 0.052  0.051 0.051 0.07    10  b 
#>        mean(x) 0.005 0.005 0.008  0.007 0.008 0.03    10 a  
```
The compiled function is around seven times faster than the uncompiled function. Of
course the native `mean` function is faster, but compiling does make a significant
difference (figure \@ref(fig:6-4)).

```{r 6-4, echo=FALSE, fig.height=4, fig.width=6, fig.cap="Comparsion of mean functions.", eval=TRUE, fig.align="center", out.width="0.7\\textwidth"}
local(source("code/03-programming_f2.R", local=TRUE))
```

### Compiling code

There are a number of ways to compile code. The easiest is to compile individual
functions using `cmpfun`, but this obviously doesn't scale. If you create a package,
you can automatically compile the package on installation by adding
```
ByteCompile: true
```
to the `DESCRIPTION` file. Most R packages installed using `install.packages` are not
compiled. We can enable (or force) packages to be compiled by starting R with the
environment variable `R_COMPILE_PKGS` set to a positive integer value and specify
that we install the package from `source`, i.e.
```{r eval=FALSE}
## Windows users will need Rtools
install.packages("ggplot2", type="source")
```
A final option to use just-in-time (JIT) compilation. The `enableJIT` function
disables JIT compilation if the argument is `0`. Arguments `1`, `2`, or `3` implement
different levels of optimisation. JIT can also be enabled by setting the environment
variable `R_ENABLE_JIT`, to one of these values.
```{block, type="rmdtip"}
I always set the compile level to the maximum value of 3.
```
